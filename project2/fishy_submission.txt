The given task was known to be a classification problem. As usual we first looked at our given input and realized that we have not only two but three labels: 0, 1 and 2, which implied that our challenge was actually a multi-class classification. Like in the last task we had 15 features with a thousand samples each.
As a first approach we tried the following algorithms: a random forest classifier, since it allows for a good accuracy and relatively fast training times; a support vector classifier, since it gives the opportunity to experiment with different kernels and a wide range of parameters to tune; and logistic regression classification, because the the model is linear and training times are fast. We also shortly experimented with a Multi Layer Perceptron Classifier, however as our computational resources were rather restrained and training times are slow we did not pursue this approach any further.
After comparing the results of the efforts mentioned above we realized that the RandomForestClassifier offers the most potential and further pursued algorithms with a decision tree approach. Since we know, that the Extra Tree Classifier allows us to further reduce the variance of our model we applied this algorithm with a grid search and cross validation to fine tune its parameters, which led us to the selected result and submission.