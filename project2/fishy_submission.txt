The given task was known to be a classification problem. As usual we first looked at our given input and realized that we have not only two but three labels: 0, 1 and 2, which implied that our challenge was actually a multi-class classification. Like in the last task we had 15 features with a thousand samples each.
As a first approach we tried the following algorithms: a random forest classifier, since it allows for a good accuracy and relatively fast training times; a support vector classifier, since it gives the opportunity to experiment with different kernels and a wide range of parameters to tune; and logistic regression classification, because the the model is linear and training times are fast. We also shortly experimented with a Multi Layer Perceptron Classifier, however as our computational resources were rather restrained and training times are slow we did not pursue this approach any further.
After comparing the results of the efforts mentioned above we realized that the RandomForestClassifier offers the most potential and further pursued algorithms with a decision tree approach. Since we know, that the Extra Tree Classifier allows us to further reduce the variance of our model we applied this algorithm with a grid search and cross validation to fine tune its parameters, which led us to the selected result and submission.


Dani submission

From the task description we know that we have a classification problem with 15 features per data point and in total 3 different classes assigned to each data point. Therefore we knew that we had to use a multiclass classification algorithm. We started by trying out a RandomForestClassifier which immediately put us somewhere close to the hard baseline, so we tried to improve this result by doing a grid search on the parameters. Additionally we tried to better our results by trying out different approaches like Support Vector Machines, Extra Trees, KNeighborsClassifiers, Logistic Regression, Gaussian Classifiers and Multilevel Perceptron Classifier. Furthermore we tried to preprocess the features with PolynomialFeatures transformation and a scaling transformation. With the SVM we tried out the different kernels. None of these approaches gave us good results, so we tried to improve the Random Forest Classifier approach by using Extra Trees. Extra Trees reduce the variance in comparison to Random Forests. In the end Extra Trees in combination with cross validation and a grid search for the best parameters led to our best result.
Only after the dead line did we try out Principle Component Analysis preprocessing in combination with SVM and immediately improved our local scores.
