The task is a classification problem, but with three classes (labeled 0,1,2). Hence we had a look at methods for multi-class classification problems.
Having thousand samples with fifteen features each, we tried out some classifiers first. Random Forest Classifier, Support Vector Classifier, Logistic Regression Classifier. Multi Layer Perceptrons were also tested, but the training time was way too big. We used a grid search on the parameter space for each method to find the most promising models. At the end it turned out that Extra Tree Classifier returned the best model. We preprocessed the data by removing the mean and scaling to unit variance (using sklearn.preprocessing.StandardScaler).

Unfortunatly I just realised after the submission deadline, that applying standard scaling in combination with Principal Component Analysis yields a major accuracy gain for Support Vector Classifiers and Extra Tree Classifiers, which prefectly makes sense since we extracted the most meaningful features with the highest covariance and try to fit seperators on a lower dimensional space.
