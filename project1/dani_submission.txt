We knew from the project description that we needed to do a Regression, so we first tried the obvious candidate: Linear Regression. This yielded unsatisfactory results, so we tried Lasso, Ridge, and Support Vector Regression to get a feel for the project. They all gave us similar results except for the SVR with a polynomial kernel. This tipped us off to the fact that the problem at hand was not linear but polynomial. After this revelation we tried different Regressions with the polynomial kernel and reached a score just shy of the hard baseline.

Since the dimension of the feature space was quite small, I wanted to try explicitly transforming the features into the higher space myself, instead of using the kernel trick. After some struggling we found the function “PolynomialFeatures” in sklearns preprocessing library which does exactly that. It explicitly transforms the features to all polynomials and combinations of the features. This gave us new 816 features instead of the original 15. We now knew that many of these features were meaningless, why we chose Lasso to eliminate as many of the useless features as possible. (Since Lasso produces sparse Soltions, i.e. where many of the wheights are 0) This resulted in our highest submission score.

During all stages we used to split the provided train set further into a validation set and a reduced train set to gain a more accurate estimation of the RMSE locally. Furthermore we used Cross Validation to find the optimal parameters.
