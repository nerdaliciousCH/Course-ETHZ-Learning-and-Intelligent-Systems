First we recognize that this is a regression task. Hence we limited ourselves to using regression-like methods.

Without further preprocessing we tried Linear Regression, Ridge Regression, Lasso, ML Perceptrons, KNN and SVR to explore all of them and their behaviours. By inspecting the results, we guessed that the data has a polynomial relationship, since purely linear models didn't yield good results. So we further used regression with a polynomial kernel which boosted us above the medium baseline. We used GridSearchCV to exhaustively search for a good model in the parameter space of the specific method with a cross validation split of 1/20. We trained the best estimator returned by GridSearchCV with 1/10 cross validation split. The score is the RMSE.

We then thought of ways to preprocess the data. We found "PolynomialFeatures" in sklearn, which generates all polynomials of using the 15 features. We chose a maximum degree of 3, e.g. for 2D features [a,b], this generates [1, a, b, ab, a^2, b^2, a^2b, ab^2, a^3, b^3]. This expands our data to 816 features. Now we have new features which express strong polyn. correlation between multiple features and now Lasso/Ridge Regression may take these into account whilst eliminating many of the newly created meaningless features, since Lasso/RR favour small weight vectors. This approach immediately gave us a local score and a public submission score beneath 15. We settled with this method.
