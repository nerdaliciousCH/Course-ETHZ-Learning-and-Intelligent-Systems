Since the task at hand was classified as a regression from the start by the project description, our efforts focused on trying different versions of regressions available to us: these were linear, ridge, lasso and support vector regression. We tried optimizing different parameters (e.g. the alpha), using scikits grid search and cross validation (GridSearchCV()) in combination with a smart train/test-split to find continuously better models and parameters and achieve a good local evaluation of our methods.

As empirically the polynomial kernel with support vector regression gave us the best score, but the original feature space was low-dimensional, we decided to try the PolynomialFeatures() function available to us to see whether we can achieve better results by generating polynomial and interaction features that were higher-dimensional. These turned out to be many, so a reduction with Lasso seemed appropriate and was worth the try, because this combination resulted in our best and selected submission result.

