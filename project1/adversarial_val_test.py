import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# adversarial validation testing
# is training and test data not really generated by the same distribution/src?
# is the test set marked different than the training set?
# --> select training examples most similar to test examples
# --> use those as validation set
# http://fastml.com/adversarial-validation-part-one/


# we train a classifier to distinguish train/test examples
# if distribution same for train & testing data, it should do no better than random (0.5)

# load data
train = pd.read_csv('train.csv')
test  = pd.read_csv('test.csv')

# assign labels for classifier
train['TARGET'] = 1
test['TARGET'] = 0

# concatenate data and shuffle
data = pd.concat((train, test))
data = data.iloc[np.random.permutation(len(data))]
data.reset_index(drop = True, inplace = True)

# extract X and Y
X = data.drop(['TARGET','Id', 'y'], axis = 1)
Y = data.TARGET

# make a new train/test split on the combined set
x_train, x_test, y_train, y_test = train_test_split(X, Y)

# now we can train and evaluate
# using AUC as score

# logistic regression
clf_lr = LogisticRegression()
clf_lr.fit(x_train,y_train)
pred_lr = clf_lr.predict(x_test)

# compute & output AUC
print "AUC for logistic regression"
print roc_auc_score(y_test, pred_lr)

# random forest 10 trees
clf_rf10 = RandomForestClassifier(n_estimators=10, n_jobs=-1)
clf_rf10.fit(x_train, y_train)
pred_rf10 = clf_rf10.predict(x_test)

# compute & output AUC
print "AUC for radom forest classifier, 10 trees"
print roc_auc_score(y_test, pred_rf10)

# random forest 100 trees
clf_rf100 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
clf_rf100.fit(x_train, y_train)
pred_rf100 = clf_rf100.predict(x_test)

# compute & output AUC
print "AUC for radom forest classifier, 100 trees"
print roc_auc_score(y_test, pred_rf100)