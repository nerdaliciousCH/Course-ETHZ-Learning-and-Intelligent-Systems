Task number three presented a multi-class classification problem and we had to predict discrete y values ranging from zero to four. We considered the potential approaches presented in the project outline and decided to tackle this problem with Tensor Flow, making use of a potential GPU speedup and the large number of neural network construction possibilities. Initially starting with the simple and available DNN-Classifier object within Tensor Flow, which implements a complete (fully connected) deep neural network that can be adapted with a myriad of parameters, we already achieved solid results. Further we systematically considered and revised the most relevant arguments to the object: the activation function, the number of hidden units and layers, the optimizer and several others. Empirically, the Adam Optimizer with its self-adapting learning rate, with three or four hidden layers and roughly 256 nodes in the first two layers in combination with the tanh activation function performed best. We then experimented with several train-test-splits and settled for the selected high score. Additional efforts to rebuild the model from ground up without the DNN Classifier were not satisfying and the approaching deadline restrained us from trying further methods.
