We knew from the task description that we needed to train a deep neural network to classify 5 different classes. Our neural network library of choice was tensorflow. First we familiarized ourself with the examples from tensorflow to get a feel for it. After that we tried the built in DNNClassifier, a fully connected deep neural network. We reached our best result with three hidden layers, the Adam optimizer algorithm, and the tanh activation function. The Adam algorithm is a optimized gradient descent algorithm to find a optimal solution. But in contrast to the "normal" gradient descent, Adam not only uses the gradient in the current point to update the weight vector, but keeps track of the last weight vector correction and factors it into the new learning step. This can help to overcome local minimas and can reach the global minima faster.
After we did not get any better scores with the built in methodes, we tried to write our own neural networks with our own hidden layer definitions. We tried to recreate the DNNClassifier solution and improve it from there. We then ran out of time and could not fully exhaust the possibilities.
