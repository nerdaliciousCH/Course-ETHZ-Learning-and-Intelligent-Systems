The task at hand is a multi class classification problem. From the many possibilities we chose TensorFlow as a platform. Initial steps were to understand how to build and run a TensorFlow neural network. Our first approach was to use the built-in DNNClassifier object. This is a fully connected deep neural network which can be parametrized by defining the number of hidden units, the activation function, the optimizer, etc. The first parameters we adjusted were the number and size of hidden units. Quickly we found that we get a huge boost by going from 2 hidden layers to 3 or even 4 layers. Afterwards we saw that increasing the number of nodes in the first 2 layers to 128/256 or even 256/512 gave another huge boost. We are not sure excatly why this worked so well, but it makes sense that having more nodes than features helps the network to encode more information like cross-correlation between features. AdamOptimizer was throughout the best optimizer and we knew that it is self-adapting it's learning rate. The activation function is TANH, which gave us the best results. This construct was trained on a 19/20 split of the training data and validated on the rest. This approach gave our high score.
Then we proceeded by building our own neural network by defining the layers ourselves in TensorFlow. We started recreating the above solution and from there wen got a score 0.03 less below our high-score with DNNClassifier. As time ran out, we missed to preprocess the data correctly.
